{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKrwy4Wv62Uw"
   },
   "source": [
    "In this project, we will use machine learning to complete a single word speech recognition task.  Here, we use one second recordings of the digits zero - nine from the Google Speech Command database https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html, meaning that every recording used belongs to one of ten classes (zero, one, two, three, four, five, six, seven, eight, or nine).  To perform the recognition, we have two frameworks as shown below:\n",
    "\n",
    "1. We can directly feed the audio signal into a neural network for audio classification.  Models that read in the raw signal with no feature extraction are called end to end models.\n",
    "\n",
    "2. We can extract some short time feature from each frame of the audio, giving the spectrogram or another acoustic representation.  We can then take these combination of these features accross time as an image and perform image classification on them.\n",
    "\n",
    "\n",
    "In this project, you will explore both methods.\n",
    "\n",
    "You may (but are not required) to change any of the code given here.  You can just add your new code to the bottom of the file.\n",
    "\n",
    "**Before you run anything, go to the toolbar at the top and select: Runtime > Change runtime type > GPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7xnFLM4ly7N"
   },
   "source": [
    "Mount your google drive to give google colab access to files stored in your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Semsm6NcUZSp",
    "outputId": "afb01570-d4b7-434e-eff0-77a1a8de4cdb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1BWoQ6r7HgB"
   },
   "source": [
    "Change directory to the place where the google ai speech commands dataset is stored.  You can find the path by typing \"!ls\" to see what files and directories are currently available.  Then type \"!ls <subdirectory you want to explore>\" to see what files are in the next subdirectory.  Keep doing that, separating the subdirectories with slashes \"/\" until you get to the speech_commands_v0.01 folder as shown below.  Then put that filepath in the os.chdir command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "ZGOjOJ1WUghi",
    "outputId": "c8719fc5-11ea-4411-d512-e318637beeb5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/content/gdrive/My Drive/speech_commands_v0.01')\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4a4cGfW7ydf"
   },
   "source": [
    "Load in the text file that lists the audio files to be used.  We'll only use the subset of files listed in this text file for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nM1j3Gd4Ugkm"
   },
   "outputs": [],
   "source": [
    "text_file = open(\"train_digit_list.txt\", \"r\")\n",
    "training_list = text_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3-iYEBD77rl"
   },
   "source": [
    "Read in all of the audio files.  Store the raw audio to a list called all_wav_list.  Store any features extracted to a listt called all_feat_list.  This part may take a while depending on what features you extract.  You can go watch netflix or work on your homework or something and come back when it's done>  You have to redo this every time you run the code which can be kind of a pain.  If you prefer, you can store the list after you've generated it and then redownload it instead of recreating it:\n",
    "\n",
    "# Import the pickle library\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Create a list\n",
    "\n",
    "l = [1,2,3,4] # create a list\n",
    "#store the list as a file\n",
    "    with open(\"test.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(l, fp)\n",
    "# open the file and read the list from it \n",
    "    with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "        b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "f414dbf40b1c45b2a8fb08b4f6f59ff4",
      "d44eda21a8414779a4a03ac554865c1c",
      "37623873f6ac4f63b6649d760a3ce107",
      "04a8a56afd244b6da6a39dc53f09db35",
      "f62c5582578a48c7a1b9b570147b4b7a",
      "c791c96fbc9646fcae392ca19d9f8004",
      "98a186a671434a63afdc6e4450b920c5",
      "c46e28d2de8240e49ac34209efb57a74"
     ]
    },
    "id": "yzzTlNeoUgpP",
    "outputId": "747306a0-5890-40cc-f7f5-82eeb8c038bd"
   },
   "outputs": [],
   "source": [
    "import librosa.feature as lf\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "from tqdm.notebook import tnrange\n",
    "\n",
    "#initialize lists\n",
    "all_wav_list = []\n",
    "all_feat_list = []\n",
    "all_labels = []\n",
    "\n",
    "#loop through all audio files listed in the text file\n",
    "for i in tnrange(len(training_list), desc='Load in files'):\n",
    "  #load in the given audio file\n",
    "  fs, audio = wav.read(training_list[i])\n",
    "\n",
    "  \n",
    "  z=np.zeros((fs,))\n",
    "  #if an audio file is less than a second, add zeros to it to make it a second\n",
    "  if audio.size<=fs:\n",
    "    z[:audio.size]=audio\n",
    "  # if an audio file is longer than a second, clip it to a second\n",
    "  elif audio.size>fs:\n",
    "    z=audio[:fs]\n",
    "  feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "  #here, we use the melspectrogram as a feature. You can use other features like\n",
    "  #LPCs, mfccs, or whatever you find.  The Librosa library has more features,\n",
    "  #and you can explore other libraries\n",
    "  all_wav_list.append(z.astype('float'))\n",
    "  all_feat_list.append(feat.reshape(1, feat.shape[0], feat.shape[1]))\n",
    "\n",
    "# get labels from the file name (ie which word is in the audio file)\n",
    "  which_word=training_list[i].split('/')[0]\n",
    "  all_labels.append(which_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irpAGci69OqE"
   },
   "source": [
    "Let's listen to one of the audio files to verify that it loaded in correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "DEmrEVWWWPDc",
    "outputId": "369906f5-bc26-4a78-f2b8-1c9718a55a52"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(training_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHoLMFx09VAi"
   },
   "source": [
    "And verify that the sizes of the lists look correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "pkNXFwqFf1w0",
    "outputId": "22097c37-6fe6-42bc-caf3-5de98b10de96"
   },
   "outputs": [],
   "source": [
    "print(len(all_wav_list))\n",
    "print(len(all_feat_list))\n",
    "print(len(all_labels))\n",
    "print(all_feat_list[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTPzv4HF9cV_"
   },
   "source": [
    "Now we'll start with speech recognition from the raw audio.  We'll concatenate the raw signals into a matrix X_sig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "AcfVtiengPlB",
    "outputId": "c4153fca-e7da-48d7-b6e7-6bf3b79ee890"
   },
   "outputs": [],
   "source": [
    "X_sig = np.vstack(all_wav_list)\n",
    "X_sig = X_sig.reshape(X_sig.shape[0], X_sig.shape[1], 1)\n",
    "print(X_sig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g1SiSOj-MY4"
   },
   "source": [
    "And we'll one hot encode our labels.  This will transform our labels from words to a vector that represents the label with a numerical value.  All of the encoded labels will have the same norm to prevent bias.  For instance, if there were five words then they would be encoded as:\n",
    "\n",
    "$y1 = [1, 0, 0, 0, 0]$\n",
    "\n",
    "$y2 = [0, 1, 0, 0, 0]$\n",
    "\n",
    "$y3 = [0, 0, 1, 0, 0]$\n",
    "\n",
    "$y4 = [0, 0, 0, 1, 0]$\n",
    "\n",
    "$y5 = [0, 0, 0, 0, 1]$\n",
    "\n",
    "This will be useful in comparing correct and incorrect labels later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyG63fh1-NLx"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import scipy as sp\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(all_labels)\n",
    "encoded_labels = le.transform(all_labels)\n",
    "\n",
    "oh_enc = preprocessing.OneHotEncoder()\n",
    "oh_enc.fit(encoded_labels.reshape(-1,1))\n",
    "\n",
    "y = oh_enc.transform(encoded_labels.reshape(-1,1))\n",
    "y = sp.sparse.csr_matrix.toarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gpoWF739rOj"
   },
   "source": [
    "Then we'll devide the whole set into a testing and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7ulgVbqhF_Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_sig_train, X_sig_test, y_train, y_test = train_test_split(X_sig, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgwtlNnfKn1U"
   },
   "source": [
    "Create and run an end to end model for audio classification:  We create a sequential model using the keras module in tensorflow and then add the desired layer to it.\n",
    "\n",
    "The graph is created of a static size, so the size of every layer must be pre-defined.  Tensorflow will take care of calculated the sizes needed in each layer if we specify the size of the input layer, the size of the output layer, and the parameters of each desired layers.\n",
    "\n",
    "The input is a collection of 1D signals each one second long.  The static graph requires that all inputs be the same size which we make sure of in shaping the input matrix above.  This will have the shape (number_of_signals, len_of_signals, 1).  The number of signals we feed into the network does not affect its structure, so we just need to specify the shapes of these inputs in input layer as (fs, 1) for the one second long signals.\n",
    "\n",
    "This input is then passed into convolutional layers and then recurrent layers (GRU) before the fully connected network (ie multilayer perceptrion) makes the final output decision.  In betweek the layers are forms of regularization such as MaxPooling (downsampling the output of a layer), dropout(randomly setting some percentage of the weights of a layer to zero for given iteration), and batch normalization (normalizing the mean and standard deviation of the output of a layer so that the numbers don't become too large).  These methods try to ensure that the solution found by the neural network does not rely too strongly on any one trend that occurs in its weights or any trend that only occurs with a small numbe of its weights.  This helps ensure that the solution found by the network is not too specific to the training data and generalizes well to any other test set of a similar distribution.\n",
    "\n",
    "At the output layer, we set the size of the last fully connected layer to 10 to let the network know that we expect a classification decision between one of the ten classes.  The output of the last layer is of the form:\n",
    "\n",
    "$out=[x_0, x_1, x_2, x_3, x_4,x_5,x_6,x_7,x_8,x_9]$\n",
    "\n",
    "where each $x_i$ is the calculated probability that the given audio sample belongs to class i.  The classification decision is then $y_{pred} =argmax_i(out)$ which we compare to the actual labels stored in the actual $y_{test}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wuO_4Uywghm4",
    "outputId": "cc03c567-1432-4945-832f-49d301dfedb8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_e2e = tf.keras.models.Sequential()\n",
    "model_e2e.add(tf.keras.Input(shape=(fs,1))) #Make sure that the input size is the size of the signal\n",
    "model_e2e.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True, )) #adjust input to unit variance and zero mean\n",
    "#First Conv1D layer\n",
    "model_e2e.add(tf.keras.layers.Conv1D(8,13, padding='valid', activation='relu', strides=1)) #Convolve with 8 1D kernels of length 13\n",
    "model_e2e.add(tf.keras.layers.MaxPooling1D(3)) #Downsample - take the max out of every three elements\n",
    "model_e2e.add(tf.keras.layers.Dropout(0.3)) #drop nodes with probability 0.3\n",
    "#Second Conv1D layer\n",
    "model_e2e.add(tf.keras.layers.Conv1D(16, 11, padding='valid', activation='relu', strides=1)) #Convolve with 16 1D kernels of length 11\n",
    "model_e2e.add(tf.keras.layers.MaxPooling1D(3))\n",
    "model_e2e.add(tf.keras.layers.Dropout(0.3))\n",
    "#Third Conv1D layer\n",
    "model_e2e.add(tf.keras.layers.Conv1D(32, 9, padding='valid', activation='relu', strides=1))\n",
    "model_e2e.add(tf.keras.layers.MaxPooling1D(3))\n",
    "model_e2e.add(tf.keras.layers.Dropout(0.3))\n",
    "model_e2e.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True))\n",
    "model_e2e.add(tf.keras.layers.Bidirectional(tf.python.keras.layers.CuDNNGRU(128, return_sequences=True), merge_mode='sum')) #Recurrent layer, uses time series data\n",
    "model_e2e.add(tf.keras.layers.Bidirectional(tf.python.keras.layers.CuDNNGRU(128, return_sequences=True), merge_mode='sum'))\n",
    "model_e2e.add(tf.keras.layers.Bidirectional(tf.python.keras.layers.CuDNNGRU(128, return_sequences=False), merge_mode='sum')) #set return sequences to False for last recurrent layer\n",
    "model_e2e.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True))\n",
    "#Flatten layer\n",
    "model_e2e.add(tf.keras.layers.Flatten()) # Turn 2D result of convolution to a single vector\n",
    "#Dense Layer 1\n",
    "model_e2e.add(tf.keras.layers.Dense(256, activation='relu')) #Fully connected layer\n",
    "model_e2e.add(tf.keras.layers. Dense(10, activation=\"softmax\")) #output layer, need size = num_classes\n",
    "model_e2e.summary() #show breakdown of parameters\n",
    "\n",
    "model_e2e.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy']) #decide loss function and metrics\n",
    "\n",
    "hist = model_e2e.fit(\n",
    "    x=X_sig_train, \n",
    "    y=y_train,\n",
    "    epochs=50, \n",
    "    batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "-VfDRKhi0fR8",
    "outputId": "aecee4ca-78fb-4b70-bf7e-4752e34ff5c2"
   },
   "outputs": [],
   "source": [
    "model_e2e.evaluate(X_sig_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o5rJbIN_39r"
   },
   "source": [
    "Now we'll move on to performing speech recognition by identifying the image of the spectrogram.  We'll begon by concatenating the features extracted into a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "VKUeL4Kqitr1",
    "outputId": "e66f5a67-f038-4ef5-d4ef-b930824ad129"
   },
   "outputs": [],
   "source": [
    "X_im = np.vstack(all_feat_list)\n",
    "X_im=np.array(X_im.reshape(X_im.shape[0],X_im.shape[1],X_im.shape[2], 1))\n",
    "print(X_im.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93yJ6lovAH9G"
   },
   "source": [
    "Let's view an image of our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "id": "srYJlxPYY6vi",
    "outputId": "caed3175-afae-47fe-d96c-523338c9f067"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "im = X_im[100,:,:].reshape(X_im.shape[1],X_im.shape[2])\n",
    "im=im[::-1,:]\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plt.imshow(im)\n",
    "plt.ylabel('frequency (bin number)')\n",
    "plt.xlabel('time (frame number)')\n",
    "plt.title('A random spectrogram');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcg5w9COAn2p"
   },
   "source": [
    "We'll similarly split our whole set into testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYpTPetTl-tv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_im_train, X_im_test, y_train, y_test = train_test_split(X_im, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdVkeqUdPmkt"
   },
   "source": [
    "Here we similarly train a CNN to recognize the images generated from the speech signals.  We again create the sequential model, and specify the input size in the first layer.\n",
    "\n",
    "The imput matrix contains a collection of 2D signals.  Here the input matrix is of the shape (number_of_images, number_of_features (or height of each image), number_of_frames (or width of each image), 1).  If these were color RGB images, then we would need to replace the 1 with a 3 to let the network know that the signals have a depth of 3.  We will stick with grayscale images for simplicity.  We tell the network to expect some number of images with a shape of (num_features, num_timesteps, 1) as the input here.\n",
    "\n",
    "This input is then passed into multiple 2D convolutional layers with maxpooling as regularization between.  This 2D maxpooling downsamples accross both the x and y dimensions of the image.\n",
    "\n",
    "The output of the last conv layer is then flattened into a 1D vector and passed to the multilayer perceptiron for a final classification decision in the same way as in the end to end model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kMI6XVfZUeM-",
    "outputId": "4d47d668-d204-40a4-e7c3-92be4a9cc334"
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_features = X_im.shape[1]\n",
    "time_steps = X_im.shape[2]\n",
    "\n",
    "model_cnn = tf.keras.models.Sequential()\n",
    "model_cnn.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(n_features, time_steps, 1))) # convolve with 32 kernels of size 3 x 3\n",
    "model_cnn.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) # convolve with 32 kernels of size 3 x 3\n",
    "model_cnn.add(tf.keras.layers.MaxPooling2D((2, 2))) #Dowmsample by 2 in each direction- take max element of every 2\n",
    "model_cnn.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) # convolve with 64 kernels of size 3 x 3\n",
    "model_cnn.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_cnn.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) # convolve with 128 kernels of size 3 x 3\n",
    "model_cnn.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_cnn.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(tf.keras.layers.Flatten()) # Flatten output into a vector\n",
    "model_cnn.add(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform')) # Fully connected layer with 128 nodes\n",
    "model_cnn.add(tf.keras.layers.Dense(10, activation='softmax')) #output layer, size must equal the number of classes\n",
    "\n",
    "Adam=tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.9, amsgrad=False)\n",
    "\n",
    "model_cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam,\n",
    "              metrics=['accuracy'])\n",
    "model_cnn.summary() #print out breakdown of model parameters\n",
    "history=model_cnn.fit(X_im_train, y_train, validation_split = 0.2, batch_size=200, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "7QKD5RvatI1U",
    "outputId": "aff035c1-9c6e-4c74-9090-a2a9101695ff"
   },
   "outputs": [],
   "source": [
    "model_cnn.evaluate(X_im_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWoBr5dqRaFH"
   },
   "source": [
    "Now that we've run this experiment on clean, noise-free signals taken with high quality microphones, and recorded by speakers all with similar speaking styles and accents, let's explore a less ideal case.  In real life, you can rarely expect the input speech to be of such high quality and need to prepare to handle common degradations in the signals.  There could be additive, noise in the background, there could be multiple speakers talking over each other, a speaker could have an accent not present in the training data, etc..  We need to be creative in how we deal with these challenges.  Here we'll introduce another common problem in audio processing: Reverberation.  Reverberations, or echos, occur when not only the original signal reaches the recording device, but also reflections of the soundwave off of nearby surfaces are directed at the microphone and also appear in the signal.  This can be modeled as multiple delayed and attenuated versions of the original signal being added back into the original signal through convolution with a room impulse function.  For this reason, we often refer to reverberations as convolutional noise, as opposed to addative noise which is added on top of the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tsSq8IPSQWE"
   },
   "source": [
    "Install the pyroomacoustics library.  It is a helpful python library that deals with calculating the effects of echos in a given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "YTwoX2oBCbzN",
    "outputId": "26dbacf8-ffd5-45e6-b7ea-b23bf16f99d3"
   },
   "outputs": [],
   "source": [
    "!pip install pyroomacoustics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXMy3401TDFN"
   },
   "source": [
    "Read in a new test set from another text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1i4OVDjq_9EN"
   },
   "outputs": [],
   "source": [
    "text_file = open(\"testing_digit_list.txt\", \"r\")\n",
    "testing_list = text_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6WbLXSsTIfQ"
   },
   "source": [
    "Add reverberations to the test signals using the pyroom acoustic library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "b80334ff3c5e417fbf39336fc2126770",
      "4c2d415c15844c578a008eb160faec17",
      "80136fb00f4344c1b7f36a8912d96074",
      "68d4d32d2b9a48818543d4caf0526a00",
      "5398b038ff7d45ca8068d15951129feb",
      "fcad123f9646492093f9df4db3df33c4",
      "5ab60d30986b4708b9571da01a98c5dd",
      "13f7954d5bb44bbdbc814a12cd34d771"
     ]
    },
    "id": "1Cc9oD9ZAuHH",
    "outputId": "fdba1d5c-cd0b-40dd-d9b2-0b974588754b"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange\n",
    "import pyroomacoustics as pra\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import librosa.feature as lf\n",
    "\n",
    "all_reverb_signals =[]\n",
    "all_reverb_feat=[]\n",
    "trial_labels = []\n",
    "\n",
    "# The desired reverberation time and dimensions of the room\n",
    "rt60 = 1.0  # seconds\n",
    "room_dim = [20, 30, 10]  # meters\n",
    "\n",
    "e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)\n",
    "\n",
    "# Create the room\n",
    "for i in tnrange(len(testing_list)):\n",
    "\n",
    "  fs, audio = wav.read(testing_list[i])\n",
    "  room = pra.ShoeBox(\n",
    "      room_dim, fs=fs, materials=pra.Material(e_absorption), max_order=max_order\n",
    "  )\n",
    "\n",
    "  # place the source in the room\n",
    "  room.add_source([2.5, 3.73, 1.76], signal=audio, delay=1.3)\n",
    "\n",
    "  mic_locs = np.c_[\n",
    "      [10, 1, 1], \n",
    "  ]\n",
    "\n",
    "  # finally place the array in the room\n",
    "  room.add_microphone_array(mic_locs)\n",
    "\n",
    "  # Run the simulation (this will also build the RIR automatically)\n",
    "  room.simulate()\n",
    "\n",
    "  mics_signals = room.mic_array.signals\n",
    "  mics_signals = mics_signals.reshape(mics_signals.size,)\n",
    "  z=mics_signals[int(1.5*fs):int(2.5*fs)]\n",
    "\n",
    "  feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "  all_reverb_signals.append(z.astype('float'))\n",
    "  all_reverb_feat.append(feat.reshape(1, feat.shape[0], feat.shape[1]))\n",
    "\n",
    "  # get labels from the file name (ie which word is in the audio file)\n",
    "  which_word=testing_list[i].split('/')[0]\n",
    "  trial_labels.append(which_word)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at_ebq7YTQ-S"
   },
   "source": [
    "Encode the labels from the new dataset using the same encoding scheme used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PruJymKkFcbT"
   },
   "outputs": [],
   "source": [
    "\n",
    "le.fit(trial_labels)\n",
    "\n",
    "encoded_labels = le.transform(trial_labels)\n",
    "\n",
    "oh_enc = preprocessing.OneHotEncoder()\n",
    "oh_enc.fit(encoded_labels.reshape(-1,1))\n",
    "\n",
    "y_trial = oh_enc.transform(encoded_labels.reshape(-1,1))\n",
    "y_trial = sp.sparse.csr_matrix.toarray(y_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QWUaTXiBUdtm",
    "outputId": "27d28c98-a399-431d-e5d7-50c7296161b0"
   },
   "outputs": [],
   "source": [
    "print(len(all_reverb_signals),len(all_reverb_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB1cFc-hTXLr"
   },
   "source": [
    "Listen to an audio signal with reverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "XJUFVnilCKsM",
    "outputId": "3a8c8857-7784-4840-c8c1-c10eb000ce9c"
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(all_reverb_signals[0], rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7BInPVUTaQn"
   },
   "source": [
    "Confirm the size of the test signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "geypTPIe9gRL",
    "outputId": "da42e8bd-f94f-4917-d176-ba8e61d37405"
   },
   "outputs": [],
   "source": [
    "X_sig_rev=np.vstack(all_reverb_signals)\n",
    "X_sig_rev=X_sig_rev.reshape(X_sig_rev.shape[0],X_sig_rev.shape[1],1)\n",
    "print(X_sig_rev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "WVDmJIV0U34-",
    "outputId": "db5c1dc0-5c74-45f7-9979-1e15d663b134"
   },
   "outputs": [],
   "source": [
    "X_im_rev = np.vstack(all_reverb_feat)\n",
    "X_im_rev=X_im_rev.reshape(X_im_rev.shape[0],X_im_rev.shape[1],X_im_rev.shape[2], 1)\n",
    "print(all_reverb_feat[0].shape)\n",
    "print(X_im_rev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAioOdcjTd0X"
   },
   "source": [
    "View an image with reverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "id": "dBXjY1CqDFfZ",
    "outputId": "f8325860-d096-43d3-de4b-e045cb9e5eed"
   },
   "outputs": [],
   "source": [
    "im = X_im_rev[1,:,:].reshape(X_im_rev.shape[1],X_im_rev.shape[2])\n",
    "im=im[::-1,:]\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plt.imshow(im)\n",
    "plt.ylabel('frequency (bin number)')\n",
    "plt.xlabel('time (frame number)')\n",
    "plt.title('A random spectrogram with reverb');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svJ9Ay5RThQb"
   },
   "source": [
    "Now evaluate the performance of the models on the test sets with reverb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "80YaTqHiYjpA",
    "outputId": "3772c7fe-1a7b-4951-fff8-1870beaff455"
   },
   "outputs": [],
   "source": [
    "model_e2e.evaluate(X_sig_rev, y_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "hJDoyo7y9UJY",
    "outputId": "245dc8f3-2cec-4dfc-8d12-b627d1f44aa8"
   },
   "outputs": [],
   "source": [
    "model_cnn.evaluate(X_im_rev, y_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mVOD7hUTm_M"
   },
   "source": [
    "We notice that the performance has worsened drastically with this degradation added to the signal  Your job in this project is to apply preprocessing to the reverberated signals as to give better performance in the networks.  Without changing the overall structure of the networks, process the reverberated signals to be more inteligible to a machine.  Think about what trends a machine will learn from the data and bring out those trends in the new test set.  Perform this for both networks.  For the end to end network, you should directly enhance the audio signals.  For the CNN, you can process the raw audio and then make your choice of feature (spectrogram, mfccs, lpcs, etc.) from the librosa library or any other library.  You can also concatenate features.  For this network, you may change the size of the input layer and retrain if your new features are of a different size than the original.  You can then apply image processing to the resulting images to the resulting image to enchance its ability to be classified correctly.  For each network, try at least 5 different processing techniques on the input signal.  Also try some combinations of those techniques and see if the combination of techniques is better or worse then the application of those techniques alone.  Submit all code at the bottom of this notebook.\n",
    "\n",
    "Note that you may not:\n",
    "1. Use additional audio files in training\n",
    "2. Introduce any overlap between the files used for training and those used for testing\n",
    "3. Add layers to any network (removing layers is fine)\n",
    "4. Assume any knowledge about the room impulse generated or add the same reverb to any training data.\n",
    "\n",
    "You should apply your knowledge of signal processing to enhance the reverberated signals for use by the neural network."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ECE_114_project_F2020",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04a8a56afd244b6da6a39dc53f09db35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c46e28d2de8240e49ac34209efb57a74",
      "placeholder": "​",
      "style": "IPY_MODEL_98a186a671434a63afdc6e4450b920c5",
      "value": " 2494/2494 [55:12&lt;00:00,  1.33s/it]"
     }
    },
    "13f7954d5bb44bbdbc814a12cd34d771": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37623873f6ac4f63b6649d760a3ce107": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Load in files: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c791c96fbc9646fcae392ca19d9f8004",
      "max": 2494,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f62c5582578a48c7a1b9b570147b4b7a",
      "value": 2494
     }
    },
    "4c2d415c15844c578a008eb160faec17": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5398b038ff7d45ca8068d15951129feb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5ab60d30986b4708b9571da01a98c5dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68d4d32d2b9a48818543d4caf0526a00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13f7954d5bb44bbdbc814a12cd34d771",
      "placeholder": "​",
      "style": "IPY_MODEL_5ab60d30986b4708b9571da01a98c5dd",
      "value": " 2552/2552 [30:45&lt;00:00,  1.38it/s]"
     }
    },
    "80136fb00f4344c1b7f36a8912d96074": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcad123f9646492093f9df4db3df33c4",
      "max": 2552,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5398b038ff7d45ca8068d15951129feb",
      "value": 2552
     }
    },
    "98a186a671434a63afdc6e4450b920c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b80334ff3c5e417fbf39336fc2126770": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_80136fb00f4344c1b7f36a8912d96074",
       "IPY_MODEL_68d4d32d2b9a48818543d4caf0526a00"
      ],
      "layout": "IPY_MODEL_4c2d415c15844c578a008eb160faec17"
     }
    },
    "c46e28d2de8240e49ac34209efb57a74": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c791c96fbc9646fcae392ca19d9f8004": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d44eda21a8414779a4a03ac554865c1c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f414dbf40b1c45b2a8fb08b4f6f59ff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_37623873f6ac4f63b6649d760a3ce107",
       "IPY_MODEL_04a8a56afd244b6da6a39dc53f09db35"
      ],
      "layout": "IPY_MODEL_d44eda21a8414779a4a03ac554865c1c"
     }
    },
    "f62c5582578a48c7a1b9b570147b4b7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fcad123f9646492093f9df4db3df33c4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
