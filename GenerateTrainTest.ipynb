{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path('./')\n",
    "data_dir = current_dir / 'speech_commands_v0.01'\n",
    "train_file_path = data_dir / \"train_digit_list.txt\"\n",
    "test_file_path = data_dir / \"testing_digit_list.txt\"\n",
    "\n",
    "train_file = open(train_file_path, \"r\")\n",
    "training_list = [data_dir / x for x in train_file.read().splitlines()]\n",
    "\n",
    "test_file = open(test_file_path, \"r\")\n",
    "testing_list = [data_dir / x for x in test_file.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.feature as lf\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "from tqdm.notebook import tnrange\n",
    "\n",
    "#initialize lists\n",
    "all_train_wav_list = []\n",
    "all_train_feat_list = []\n",
    "all_train_labels = []\n",
    "\n",
    "#loop through all audio files listed in the text file\n",
    "for i in tnrange(len(training_list), desc='Load in files'):\n",
    "    #load in the given audio file\n",
    "    fs, audio = wav.read(training_list[i])\n",
    "#     print(i, training_list[i])\n",
    "\n",
    "\n",
    "    z=np.zeros((fs,))\n",
    "    #if an audio file is less than a second, add zeros to it to make it a second\n",
    "    if audio.size<=fs:\n",
    "        z[:audio.size]=audio\n",
    "    # if an audio file is longer than a second, clip it to a second\n",
    "    elif audio.size>fs:\n",
    "        z=audio[:fs]\n",
    "    feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "    #here, we use the melspectrogram as a feature. You can use other features like\n",
    "    #LPCs, mfccs, or whatever you find.  The Librosa library has more features,\n",
    "    #and you can explore other libraries\n",
    "    all_train_wav_list.append(z.astype('float'))\n",
    "    all_train_feat_list.append(feat.reshape(1, feat.shape[0], feat.shape[1]))\n",
    "\n",
    "    # get labels from the file name (ie which word is in the audio file)\n",
    "    which_word=str(training_list[i].parent.name)\n",
    "    all_train_labels.append(which_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize lists\n",
    "all_test_wav_list = []\n",
    "all_test_feat_list = []\n",
    "all_test_labels = []\n",
    "\n",
    "#loop through all audio files listed in the text file\n",
    "for i in tnrange(len(testing_list), desc='Load in files'):\n",
    "  #load in the given audio file\n",
    "  fs, audio = wav.read(testing_list[i])\n",
    "\n",
    "  \n",
    "  z=np.zeros((fs,))\n",
    "  #if an audio file is less than a second, add zeros to it to make it a second\n",
    "  if audio.size<=fs:\n",
    "    z[:audio.size]=audio\n",
    "  # if an audio file is longer than a second, clip it to a second\n",
    "  elif audio.size>fs:\n",
    "    z=audio[:fs]\n",
    "  feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "  #here, we use the melspectrogram as a feature. You can use other features like\n",
    "  #LPCs, mfccs, or whatever you find.  The Librosa library has more features,\n",
    "  #and you can explore other libraries\n",
    "  all_test_wav_list.append(z.astype('float'))\n",
    "  all_test_feat_list.append(feat.reshape(1, feat.shape[0], feat.shape[1]))\n",
    "\n",
    "# get labels from the file name (ie which word is in the audio file)\n",
    "  which_word=str(testing_list[i].parent.name)\n",
    "  all_test_labels.append(which_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate original signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import scipy as sp\n",
    "\n",
    "X_train_sig = np.vstack(all_train_wav_list)\n",
    "X_train_sig = X_train_sig.reshape(X_train_sig.shape[0], X_train_sig.shape[1], 1)\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(all_train_labels)\n",
    "encoded_labels = le.transform(all_train_labels)\n",
    "\n",
    "oh_enc = preprocessing.OneHotEncoder()\n",
    "oh_enc.fit(encoded_labels.reshape(-1,1))\n",
    "\n",
    "y_train = oh_enc.transform(encoded_labels.reshape(-1,1))\n",
    "y_train = sp.sparse.csr_matrix.toarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train_original.npy\", X_train_sig)\n",
    "np.save(\"y_train.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sig = np.vstack(all_test_wav_list)\n",
    "X_test_sig = X_test_sig.reshape(X_test_sig.shape[0], X_test_sig.shape[1], 1)\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(all_test_labels)\n",
    "encoded_labels = le.transform(all_test_labels)\n",
    "\n",
    "oh_enc = preprocessing.OneHotEncoder()\n",
    "oh_enc.fit(encoded_labels.reshape(-1,1))\n",
    "\n",
    "y_test = oh_enc.transform(encoded_labels.reshape(-1,1))\n",
    "y_test = sp.sparse.csr_matrix.toarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_test_original.npy\", X_train_sig)\n",
    "np.save(\"y_test.npy\", y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate echo signal from one configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange\n",
    "import pyroomacoustics as pra\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import librosa.feature as lf\n",
    "\n",
    "all_train_reverb_signals =[]\n",
    "all_train_reverb_feat=[]\n",
    "trial_train_labels = []\n",
    "\n",
    "# The desired reverberation time and dimensions of the room\n",
    "rt60 = 1.0  # seconds\n",
    "room_dim = [20, 30, 10]  # meters\n",
    "\n",
    "e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)\n",
    "\n",
    "# Create the room\n",
    "for i in tnrange(len(training_list)):\n",
    "\n",
    "  fs, audio = wav.read(training_list[i])\n",
    "  room = pra.ShoeBox(\n",
    "      room_dim, fs=fs, materials=pra.Material(e_absorption), max_order=max_order\n",
    "  )\n",
    "\n",
    "  # place the source in the room\n",
    "  room.add_source([2.5, 3.73, 1.76], signal=audio, delay=1.3)\n",
    "\n",
    "  mic_locs = np.c_[\n",
    "      [10, 1, 1], \n",
    "  ]\n",
    "\n",
    "  # finally place the array in the room\n",
    "  room.add_microphone_array(mic_locs)\n",
    "\n",
    "  # Run the simulation (this will also build the RIR automatically)\n",
    "  room.simulate()\n",
    "\n",
    "  mics_signals = room.mic_array.signals\n",
    "  mics_signals = mics_signals.reshape(mics_signals.size,)\n",
    "  z=mics_signals[int(1.5*fs):int(2.5*fs)]\n",
    "\n",
    "  feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "  all_train_reverb_signals.append(z.astype('float'))\n",
    "  all_train_reverb_feat.append(feat.reshape(1, feat.shape[0], feat.shape[1]))\n",
    "\n",
    "  # get labels from the file name (ie which word is in the audio file)\n",
    "  which_word=training_list[i].parent.name\n",
    "  trial_train_labels.append(which_word)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reverb_sig = np.vstack(all_train_reverb_signals)\n",
    "X_train_reverb_sig = X_train_reverb_sig.reshape(X_train_reverb_sig.shape[0], X_train_reverb_sig.shape[1], 1)\n",
    "print(X_train_reverb_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train_reverb.npy\", X_train_reverb_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_reverb_signals =[]\n",
    "all_test_reverb_feat=[]\n",
    "trial_test_labels = []\n",
    "\n",
    "# The desired reverberation time and dimensions of the room\n",
    "rt60 = 1.0  # seconds\n",
    "room_dim = [20, 30, 10]  # meters\n",
    "\n",
    "e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)\n",
    "\n",
    "# Create the room\n",
    "for i in tnrange(len(testing_list)):\n",
    "\n",
    "  fs, audio = wav.read(testing_list[i])\n",
    "  room = pra.ShoeBox(\n",
    "      room_dim, fs=fs, materials=pra.Material(e_absorption), max_order=max_order\n",
    "  )\n",
    "\n",
    "  # place the source in the room\n",
    "  room.add_source([2.5, 3.73, 1.76], signal=audio, delay=1.3)\n",
    "\n",
    "  mic_locs = np.c_[\n",
    "      [10, 1, 1], \n",
    "  ]\n",
    "\n",
    "  # finally place the array in the room\n",
    "  room.add_microphone_array(mic_locs)\n",
    "\n",
    "  # Run the simulation (this will also build the RIR automatically)\n",
    "  room.simulate()\n",
    "\n",
    "  mics_signals = room.mic_array.signals\n",
    "  mics_signals = mics_signals.reshape(mics_signals.size,)\n",
    "  z=mics_signals[int(1.5*fs):int(2.5*fs)]\n",
    "\n",
    "  feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "  all_test_reverb_signals.append(z.astype('float'))\n",
    "  all_test_reverb_feat.append(feat.reshape(1, feat.shape[0], feat.shape[1]))\n",
    "\n",
    "  # get labels from the file name (ie which word is in the audio file)\n",
    "  which_word=testing_list[i].parent.name\n",
    "  trial_test_labels.append(which_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reverb_sig = np.vstack(all_test_reverb_signals)\n",
    "X_test_reverb_sig = X_test_reverb_sig.reshape(X_test_reverb_sig.shape[0], X_test_reverb_sig.shape[1], 1)\n",
    "print(X_test_reverb_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_test_reverb.npy\", X_test_reverb_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate echo signal from random configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange\n",
    "import pyroomacoustics as pra\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import librosa.feature as lf\n",
    "\n",
    "all_train_reverb_random_signals =[]\n",
    "all_train_reverb_random_feat=[]\n",
    "\n",
    "# The desired reverberation time and dimensions of the room\n",
    "rt60 = 1.0  # seconds\n",
    "room_dim = [20, 30, 10]  # meters\n",
    "\n",
    "e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)\n",
    "\n",
    "# Create the room\n",
    "for i in tnrange(len(training_list)):\n",
    "\n",
    "  fs, audio = wav.read(training_list[i])\n",
    "  room = pra.ShoeBox(\n",
    "      room_dim, fs=fs, materials=pra.Material(e_absorption), max_order=max_order\n",
    "  )\n",
    "\n",
    "  # place the source in the room\n",
    "  room.add_source([np.random.uniform(1,19), np.random.uniform(1,29), np.random.uniform(1,9)], signal=audio, delay=1.3)\n",
    "\n",
    "  mic_locs = np.c_[\n",
    "      [10, 1, 1], \n",
    "  ]\n",
    "\n",
    "  # finally place the array in the room\n",
    "  room.add_microphone_array(mic_locs)\n",
    "\n",
    "  # Run the simulation (this will also build the RIR automatically)\n",
    "  room.simulate()\n",
    "\n",
    "  mics_signals = room.mic_array.signals\n",
    "  mics_signals = mics_signals.reshape(mics_signals.size,)\n",
    "  z=mics_signals[int(1.5*fs):int(2.5*fs)]\n",
    "\n",
    "  feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "  all_train_reverb_random_signals.append(z.astype('float'))\n",
    "  all_train_reverb_random_feat.append(feat.reshape(1, feat.shape[0], feat.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reverb_random_sig = np.vstack(all_train_reverb_random_signals)\n",
    "X_train_reverb_random_sig = X_train_reverb_sig.reshape(X_train_reverb_random_sig.shape[0], X_train_reverb_random_sig.shape[1], 1)\n",
    "print(X_train_reverb_random_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train_reverb_random.npy\", X_train_reverb_random_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange\n",
    "import pyroomacoustics as pra\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import librosa.feature as lf\n",
    "\n",
    "all_test_reverb_random_signals =[]\n",
    "all_test_reverb_random_feat=[]\n",
    "\n",
    "# The desired reverberation time and dimensions of the room\n",
    "rt60 = 1.0  # seconds\n",
    "room_dim = [20, 30, 10]  # meters\n",
    "\n",
    "e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)\n",
    "\n",
    "# Create the room\n",
    "for i in tnrange(len(testing_list)):\n",
    "\n",
    "  fs, audio = wav.read(testing_list[i])\n",
    "  room = pra.ShoeBox(\n",
    "      room_dim, fs=fs, materials=pra.Material(e_absorption), max_order=max_order\n",
    "  )\n",
    "\n",
    "  # place the source in the room\n",
    "  room.add_source([np.random.uniform(1,19), np.random.uniform(1,29), np.random.uniform(1,9)], signal=audio, delay=1.3)\n",
    "\n",
    "  mic_locs = np.c_[\n",
    "      [10, 1, 1], \n",
    "  ]\n",
    "\n",
    "  # finally place the array in the room\n",
    "  room.add_microphone_array(mic_locs)\n",
    "\n",
    "  # Run the simulation (this will also build the RIR automatically)\n",
    "  room.simulate()\n",
    "\n",
    "  mics_signals = room.mic_array.signals\n",
    "  mics_signals = mics_signals.reshape(mics_signals.size,)\n",
    "  z=mics_signals[int(1.5*fs):int(2.5*fs)]\n",
    "\n",
    "  feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "  all_test_reverb_random_signals.append(z.astype('float'))\n",
    "  all_test_reverb_random_feat.append(feat.reshape(1, feat.shape[0], feat.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reverb_random_sig = np.vstack(all_test_reverb_random_signals)\n",
    "X_test_reverb_random_sig = X_test_reverb_sig.reshape(X_test_reverb_random_sig.shape[0], X_test_reverb_random_sig.shape[1], 1)\n",
    "print(X_test_reverb_random_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_test_reverb_random.npy\", X_test_reverb_random_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(X_test_reverb_random_sig.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate echo from random room configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange\n",
    "import pyroomacoustics as pra\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import librosa.feature as lf\n",
    "\n",
    "all_train_reverb_random_signals =[]\n",
    "all_train_reverb_random_feat=[]\n",
    "\n",
    "\n",
    "# Create the room\n",
    "for i in tnrange(len(training_list)):\n",
    "\n",
    "    rt60 = np.random.uniform(0.5, 1.0)\n",
    "    x = np.random.uniform(10, 20)\n",
    "    y = np.random.uniform(10, 30)\n",
    "    z = np.random.uniform(5, 10)\n",
    "    room_dim = [x, y, z]  # meters\n",
    "    e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)\n",
    "\n",
    "    fs, audio = wav.read(training_list[i])\n",
    "    room = pra.ShoeBox(\n",
    "    room_dim, fs=fs, materials=pra.Material(e_absorption), max_order=max_order\n",
    "    )\n",
    "\n",
    "    # place the source in the room\n",
    "    room.add_source([np.random.uniform(1,x-1), np.random.uniform(1,y-1), np.random.uniform(1,z-1)], signal=audio, delay=1.3)\n",
    "\n",
    "    mic_locs = np.c_[\n",
    "    [10, 1, 1], \n",
    "    ]\n",
    "\n",
    "    # finally place the array in the room\n",
    "    room.add_microphone_array(mic_locs)\n",
    "\n",
    "    # Run the simulation (this will also build the RIR automatically)\n",
    "    room.simulate()\n",
    "\n",
    "    mics_signals = room.mic_array.signals\n",
    "    mics_signals = mics_signals.reshape(mics_signals.size,)\n",
    "    z=mics_signals[int(1.5*fs):int(2.5*fs)]\n",
    "\n",
    "    feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "    all_train_reverb_random_signals.append(z.astype('float'))\n",
    "    all_train_reverb_random_feat.append(feat.reshape(1, feat.shape[0], feat.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_train_reverb_random_signals[410])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_train_reverb_random_signals)):\n",
    "    length = len(all_train_reverb_random_signals[i])\n",
    "#     print(length)\n",
    "    if length != 16000:\n",
    "        print(i)\n",
    "        arr = all_train_reverb_random_signals[i].copy()\n",
    "        zeros = np.full(16000 - length, 0.0e+00)\n",
    "        all_train_reverb_random_signals[i] = np.concatenate((arr, zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reverb_random_sig = np.vstack(all_train_reverb_random_signals)\n",
    "X_train_reverb_random_sig = X_train_reverb_random_sig.reshape(X_train_reverb_random_sig.shape[0], X_train_reverb_random_sig.shape[1], 1)\n",
    "print(X_train_reverb_random_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train_reverb_room_random.npy\", X_train_reverb_random_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange\n",
    "import pyroomacoustics as pra\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import librosa.feature as lf\n",
    "\n",
    "all_test_reverb_random_signals =[]\n",
    "all_test_reverb_random_feat=[]\n",
    "\n",
    "\n",
    "\n",
    "# Create the room\n",
    "for i in tnrange(len(testing_list)):\n",
    "    \n",
    "    # The desired reverberation time and dimensions of the room\n",
    "    rt60 = np.random.uniform(0.5, 1.0)\n",
    "    x = np.random.uniform(10, 20)\n",
    "    y = np.random.uniform(10, 30)\n",
    "    z = np.random.uniform(5, 10)\n",
    "    room_dim = [x, y, z]  # meters\n",
    "\n",
    "    e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)\n",
    "\n",
    "    fs, audio = wav.read(testing_list[i])\n",
    "    room = pra.ShoeBox(\n",
    "      room_dim, fs=fs, materials=pra.Material(e_absorption), max_order=max_order\n",
    "    )\n",
    "\n",
    "    # place the source randomly in the room\n",
    "    room.add_source([np.random.uniform(1,x-1), np.random.uniform(1,y-1), np.random.uniform(1,z-1)], signal=audio, delay=1.3)\n",
    "\n",
    "    mic_locs = np.c_[\n",
    "      [10, 1, 1], \n",
    "    ]\n",
    "\n",
    "    # finally place the array in the room\n",
    "    room.add_microphone_array(mic_locs)\n",
    "\n",
    "    # Run the simulation (this will also build the RIR automatically)\n",
    "    room.simulate()\n",
    "\n",
    "    mics_signals = room.mic_array.signals\n",
    "    mics_signals = mics_signals.reshape(mics_signals.size,)\n",
    "    z=mics_signals[int(1.5*fs):int(2.5*fs)]\n",
    "\n",
    "    feat = lf.melspectrogram(z.astype('float'), sr =fs)\n",
    "    all_test_reverb_random_signals.append(z.astype('float'))\n",
    "    all_test_reverb_random_feat.append(feat.reshape(1, feat.shape[0], feat.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_test_reverb_random_signals)):\n",
    "    length = len(all_test_reverb_random_signals[i])\n",
    "#     print(length)\n",
    "    if length != 16000:\n",
    "        arr = all_test_reverb_random_signals[i].copy()\n",
    "        zeros = np.full(16000 - length, 0.0e+00)\n",
    "        all_test_reverb_random_signals[i] = np.concatenate((arr, zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reverb_random_sig = np.vstack(all_test_reverb_random_signals)\n",
    "X_test_reverb_random_sig = X_test_reverb_random_sig.reshape(X_test_reverb_random_sig.shape[0], X_test_reverb_random_sig.shape[1], 1)\n",
    "print(X_test_reverb_random_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_test_reverb_room_random.npy\", X_test_reverb_random_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
